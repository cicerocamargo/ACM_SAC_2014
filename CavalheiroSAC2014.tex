\documentclass{sig-alternate}
  \pdfpagewidth=8.5truein
 \pdfpageheight=11truein


%\usepackage{algorithm2e}
\usepackage[norelsize]{algorithm2e}


\newtheorem{thm}{Theorem}
\newtheorem{definition}[thm]{Definition}

\begin{document}
%
% --- Author Metadata here ---

% \conferenceinfo{SAC'13}{March 18-22, 2013, Coimbra, Portugal.}
% \CopyrightYear{2013} % Allows default copyright year (2002) to be over-ridden - IF NEED BE.
% \crdata{978-1-4503-1656-9/13/03}  % Allows default copyright data (X-XXXXX-XX-X/XX/XX) to be over-ridden.

% --- End of Author Metadata ---

\title{Title Present!}

\numberofauthors{3} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Gerson G. H. Cavalheiro\titlenote{Thanks to?}\\
       \affaddr{Federal Univeristy Of Pelotas}\\
       \affaddr{Gomes Carneiro, 1}\\
       \affaddr{Pelotas, Brazil}\\
       \email{gerson.cavalheiro@inf.ufpel.edu.br}
% 2nd. author
\alignauthor Alan S. de Ara{\'u}jo\titlenote{Thanks to?}\\
       \affaddr{Federal Univeristy Of Pelotas}\\
       \affaddr{Gomes Carneiro, 1}\\
       \affaddr{Pelotas, Brazil}\\
       \email{asdaraujo@inf.ufpel.edu.br}
% 3rd. author
\alignauthor C{\'i}cero A. de S. Camargo\\
       \affaddr{Federal Univeristy Of Pelotas}\\
       \affaddr{Gomes Carneiro, 1}\\
       \affaddr{Pelotas, Brazil}\\
       \email{cadscamargo@inf.ufpel.edu.br}
}


\maketitle
\begin{abstract}

Testing tex editing on GitHub! E eu consegui!

\end{abstract}

% % A category with the (minimum) three required fields
% \category{H.4}{Information Systems Applications}{Miscellaneous}
% %A category including the fourth, optional field follows...
% \category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

% \terms{Delphi theory}

\keywords{Concurrent Programming, Multithread Environment, Thread Scheduling}

%---------------------------------------------------------------------
\section{Introduction}

The acceptance of multiprocessors in different domains of application
and usages are primarily a result of the quest to increase computer
system performance. The multicore technology is nowadays a usual
solution employed from desktops to machines listed in Top 500
Supercomputer Sites. The main appeal of such architectures is the high
potential of performance added to good ratio cost/performance. Another
aspect related to the recent popularity of such systems is the
simplicity to write programs when compared to distributed memory
architecture, since it is unnecessary to deal with addresses of
processes to communicate parts of a same program. Thus, multithreading
programming is considered a convenient and efficient mechanism to
exploit multicore architectures \cite{Sodan05, mei:2010}. Nevertheless,
development of parallel applications for any multiprocessor
architecture requires the programmer ability to both to conceive and
write a concurrent/parallel program from an application description
and to design a strategy to fully utilize the computing resources.

Several commercial and academic options for programming multiprocessor
systems are available. Among the most popular commercial programming
tools we can name Pthread \cite{NicholsPthreads}, OpenMP \cite{ChandraOpenMP}, Cilk+
\cite{IntelCilkPlus} and TBB \cite{TBB2007}. Among the academic/research proposals,
we name Athreads \cite{VecParLNCS}, Filaments \cite{Filaments94}, Kaapi \cite{kaapi2007}
and Nabbit \cite{Nabbit2010}. Excepting  Pthread, which offers only an API
(Application Programming Interface) to develop a parallel program and
delegates to the operating system the scheduling of threads over the
computing resources, all cited programming tools offer both a
multithread API and a runtime supporting  a application level
(\cite{feitelson1997job}) scheduling strategy. To achieve the scheduling at
application level, the runtime must to manage the threads created by
the program during the execution, competing with the application
program for the system resources and generating overhead at execution
time. To reduce the impact of the scheduling overhead, the data
structures employed to handle the user level threads (those threads
generated by the application program) must to be implemented
efficiently.

In this paper we investigate the data structures required to
implement, at execution time, a scheduling strategy based on list
scheduling, whereas threads are inserted removed taking into account
this basic policy: ({\em i}) a thread can be added to the lists when
it has no more dependencies to execute, that is, the thread is ready
to execute, and ({\em ii}) once a processor becomes idle, some
criteria is applied to rank all threads in the list in order to chose
the  one with the grater rank (priority) to be executed at this
moment. The goal is to evaluate alternatives to implement efficient
support such data structures for a specific multithread runtime.

The runtime scheduler of Athread, Cilk+, Kaapi, Nabbit and TBB take
into account the depth of the threads to ranking the list. As far from
the first thread generated by the program as deeper the thread is
considered. We will consider in this work list scheduling which deal
whit this application propriety. Thus, an efficient implementation of
such list is essential feature to obtain good performance. Our case
study will be conduced in the context of Athread project. We propose
two alternatives to implement such list in its runtime kernel. First
we evaluate the use of optimized malloc/free internal library then a
lock-free strategy. Both strategies were developed considering the
requirements of the Athread scheduling kernel.

The remaining of the paper is organized as follows. In Section
\ref{sec:related}  we introduce the use of list scheduling as basic
strategy of scheduling on multithreading environments and we also
summarize some techniques employed in some previous listed
environments to implement the list of threads.  In Section
\ref{sec:Athread}, we detail Athread runtime and in the following, in
Section \ref{sec:implementation}, we present the two approaches
evaluated in this work. A performance assessment of these two
approaches is presented in Section \ref{sec:evaluation}. Finally,
final remarks and future work are discussed in \ref{sec:conclusion}.

%-------------------------------------------------------------
\section{Related Work}\label{sec:related}

A large number of scheduling heuristics algorithms are based on list
scheduling strategy [este, 2, 3, 4, 7]. Consequently, we found many
runtime systems employing dynamic list scheduling techniques at
runtime. In this section we briefly present this strategy and 
data structures required to support the scheduling
operations. We also present the employ of list on Cilk, Kaapi and TBB.

%-------------------------------------------------------------
\subsection{Dynamic list scheduling}\label{ssec:dls}

A dynamic list scheduling takes as input a DAG (Directed Ciclic Graph)
describing task and dependencies among tasks \cite{Graham66}. In this
DAG vertices represent the tasks and the edges represent the
communication among tasks. A data produced (output) of a given task
$v_i$ and necessary (input) to compute another task $v_j$ is
represented as an edge $e_{i,j}$ in the DAG and consist in a
dependence $v_i{\rightarrow}v_j$. In other worlds, $v_j$ (called sink
task) is not ready to execute before $v_i$ (called source task) ends.
The list of immediate successors or predecessors of a task $v_i$ is
given by $succ(v_i)$ and $pred(v_i)$, respectively. If a task has not
any predecessor it represents an {\em entry} task, as well as
represents a {\em exit} task when has no successors.The vertices and
edges in the DAG can be also annotated at application level with costs
required to compute the tasks and/or the communicate the data,
respectively.

\begin{definition}[Task]
A task $v_i$ of a given DAG $G$ is ready to execute when all tasks in
$pred(v_i)$ were finished; once a task is started, it must to be
executed until completion. A running task can not be neither preempted
nor migrated.
\end{definition}

This task model implies that a processor is able to execute a task at
once. As result, if the number $n$ of ready tasks is larger than the
number $m$ of processors, ready tasks must to be maintained in data
structure waiting for its launching. Regardless specific
implementation, we assume this data structure is a {\em list} globally
accessible to all processors. For sake of simplicity we assume also a
multiprocessor machine with no costs (latencies) associated to a ask
read/write input/output data.

The basic algorithm for a dynamic list scheduling is given in
Algorithm \ref{algo:dls}. In this algorithm, a ready task is a task
having all its predecessors scheduled. The output of the scheduler is
a matrix representing the mapping of tasks onto the processors. Each
matrix line represents one of the $m$ processor of a given parallel
architecture and each column a task in the graph $G$ to be scheduled.

\begin{algorithm} \label{algo:dls}
\SetKwInput{KwData}{Input}\SetKwInput{KwResult}{Output}\SetKwComment{Comment}{\{}{\}}
\KwData{$G$: a DAG with $n$ vertices ;  $m$: the number of processors.}

\KwResult{$M[m, n]$: each line in $M$ represents a processor and each column a vertex of $G$. For a given position $M(i,j)$, a non $null$ value represent the data $d$ to start task.}

 $V \longleftarrow \emptyset$  \Comment*[r]{The list of ready tasks}
 $M(i,j) \longleftarrow null$     \Comment*[r]{For all $0\leq i \leq m, 0\leq j\le n$}
 
 \While{$G \neq \emptyset$}{
     $V \longleftarrow V \cup G.readyTasks$\Comment*[r]{Read and remove...}
     $G \longleftarrow G - G.readyTasks$  \Comment*[r]{...ready tasks}
     $V.sort$                       \Comment*[r]{Apply a ranking policy}
     $v \longleftarrow V.first$\;
     $p \longleftarrow M.nextIdleProcessor$
     $M(p,v) \longleftarrow dataStart(v)$\Comment*[f]{Maps the task and...}
  {$~$}~\Comment*[f]{...sets all $succ(v)$ as ready}
}
\caption{A general Dynamic List Algorithm scheduling algorithm.}
\end{algorithm}

\subsection{Work stealing}
  
Work stealing has proven \cite{blelloch} to be an effective method for scheduling parallel programs on multiprocessors. One of the reasons is that this approach limits the number of scheduling operations involving more than one processor to the situation were a processor has no more local task to process. When this situation happens, a steal is initiated by the idle processor to obtain work from another (randomly chosen) processor. Thus, a local scheduler operates a local partition of the list of ready tasks. a processor accesses the partition belonging to another processor only when it's own partition becomes empty.
 
Probably one of most know work steal implementation is provided by Cilk. The local lists are implemented with a double ended queue ({\em deque}). The local scheduler accesses the {\em bottom} of the queue to insert and remove tasks. In a steal, the thief accesses the {\em top} of a remote partition. This strategy allows to attain remarkable performance indexes when associated to an efficient heuristic to detect the critical path (that is, the long chain of dependencies of tasks \cite{Graham}) of the running program. In Cilk the critical path can be inferred since its API limits writing parallel programs in a nested fork-join style. In such way, closer a task is from the top of a partition, more amount of work (new tasks) it is able to create. A steal in this case potentialize the success of steal, providing a task which will makes grow the thief deque, while exploit at local scheduler the locality of tasks limiting synchronization among processors.

\subsection{Task stealing}
cicero 2 paragrafos

\subsection{Private queues}
Gerson

\subsection{Distributed list}
Gerson

%-------------------------------------------------------------
\section{Athread}\label{sec:athread}

%-------------------------------------------------------------
\section{Implementation}\label{sec:implementation}

\subsection{Non blocking data structures}
Guilherme

\subsection{Smarth lists}
Alan

\subsection{os dois}
Alan

%-------------------------------------------------------------
\section{Evaluation}\label{sec:evaluation}

%-------------------------------------------------------------
\section{Conclusion}\label{sec:conclusion}

\bibliographystyle{abbrv}
\bibliography{bibs} 

\end{document}
